---
title: "Sparse Auto Encoder"
date: 2018-03-16
tags: [Machine Learning, Sparse Auto Encoder]
excerpt: "Created a sparse autoencoder and implemented semi-supervised learning with MNIST data." 
mathjax: "true"
header:
    image: "/assets/images/Intro-to-ML-4/sae-banner.png"
---

In this project I created a sparse autoencoder and implement semi-supervised learning with MNIST data.

##  Sparse Autoencoder

[link to repository](https://github.com/AchyuthaBharadwaj/Machine-Learning/tree/master/Sparse%20Auto%20Encoder)

Architecture of the sae:

<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Intro-to-ML-4/sae-details.JPG" alt="basic sparse autoencoder architecture"/>

Dataset: MNIST - All the digits <br/>
Autoencoder parameters:<br/>
Num of hidden layers: 1<br/>
hidden layer Size = 200<br/>
max iterations = 400<br/>
Sparsity parameter $$p = [0.01, 0.1, 0.5, 0.8]$$<br/>
Weight decay $$λ = 0.001$$<br/>
Sparsity coefficient $$β = 3$$<br/>

After training the auto encoder, the mosaic of the weights I obtained is as follows;

<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Intro-to-ML-4/sae-weights.png" alt="basic sparse autoencoder weights mosaic"/>

